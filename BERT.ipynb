{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9157be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ac1877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ec6b363e94473b9480770c44b03d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5f4f669ac04093841e0e1b123d35f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:15:55.150768: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-04-09 12:15:55.150837: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-04-09 12:15:55.150857: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-04-09 12:15:55.150907: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-09 12:15:55.150936: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6839e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2bdb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 101, 7592, 2088,  102,    0,    0],\n",
       "       [ 101, 7592, 2129, 2024, 2017,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(['hello world', 'hello how are you'], padding = True, truncation = True, return_tensors='tf')\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81d5d076-4dee-4ba3-b464-0b33fd4dd877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 6, 768), dtype=float32, numpy=\n",
       "array([[[-1.68883756e-01,  1.36063516e-01, -1.39400631e-01, ...,\n",
       "         -6.25112534e-01,  5.21728694e-02,  3.67145777e-01],\n",
       "        [-3.63274485e-01,  1.41218156e-01,  8.79987717e-01, ...,\n",
       "          1.04330614e-01,  2.88757414e-01,  3.72679204e-01],\n",
       "        [-6.98594093e-01, -6.98797822e-01,  6.45025298e-02, ...,\n",
       "         -2.21036404e-01,  9.86776501e-03, -5.93979418e-01],\n",
       "        [ 8.30982983e-01,  1.23666808e-01, -1.51189953e-01, ...,\n",
       "          1.03097424e-01, -6.77926600e-01, -2.62851775e-01],\n",
       "        [-4.02666271e-01, -1.92825329e-02,  5.73250055e-01, ...,\n",
       "         -2.06568196e-01,  2.33856291e-02,  2.01263204e-01],\n",
       "        [-6.22840822e-01, -2.74535656e-01,  1.81175709e-01, ...,\n",
       "         -1.29448786e-01, -3.83912474e-02, -5.73319346e-02]],\n",
       "\n",
       "       [[ 3.53770144e-02,  4.97097336e-03, -1.63990170e-01, ...,\n",
       "         -2.94695139e-01,  1.00460052e-01,  2.69069970e-01],\n",
       "        [ 1.91546619e-01,  1.55038625e-01,  3.97726029e-01, ...,\n",
       "         -6.07962370e-01,  5.23666561e-01, -2.53920019e-01],\n",
       "        [-3.58025968e-01, -6.16344750e-01,  4.82602194e-02, ...,\n",
       "         -2.88758427e-04,  7.10645020e-01, -6.09312236e-01],\n",
       "        [ 4.14312661e-01, -1.35979056e+00,  1.86189756e-01, ...,\n",
       "         -4.94022876e-01,  4.38691586e-01, -3.78174931e-01],\n",
       "        [-2.73466021e-01, -1.25411904e+00,  3.31949234e-01, ...,\n",
       "         -5.65506667e-02,  3.85584325e-01, -5.87253571e-01],\n",
       "        [ 5.24139225e-01, -2.56137848e-01, -3.20331782e-01, ...,\n",
       "          4.12313193e-01, -3.82963508e-01, -2.16114849e-01]]],\n",
       "      dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.9061535 , -0.31115326, -0.6216542 , ..., -0.3057524 ,\n",
       "        -0.6400939 ,  0.9166175 ],\n",
       "       [-0.9134232 , -0.32655174, -0.57403576, ..., -0.4289204 ,\n",
       "        -0.63983107,  0.9282712 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b67b9a-a2fc-4979-9662-d3f76ee54417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'I love \\xf0\\x9f\\x8d\\x95! shall we book a \\xf0\\x9f\\x9a\\x96 to gizza?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I love üçï! shall we book a üöñ to gizza?'\n",
    "text = text.encode(\"utf-8\")\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18781f84-64e3-4480-857f-d6c65a19b12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love üçï! shall we book a üöñ to gizza?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = text.decode('utf-8')\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9033c1a-1575-414d-8b02-594fbe755da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We notice that there are two errors in the output of the OCR system in this case.\\nDepending on the quality of the original scan, OCR output can potentially have\\nlarger amounts of errors. How do we clean up this text before feeding it into the next\\nstage of the pipeline? One approach is to run the text through a spell checker such as\\npyenchant [28], which will identify misspellings and suggest some alternatives. More\\nrecent approaches use neural network architectures to train word/character-based\\nlanguage models, which are in turn used for correcting OCR text output based on the\\ncontext [29].\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "filename = \"screenshot.png\"\n",
    "text = image_to_string(Image.open(filename))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74558b56-2a09-4694-ac30-b5ff82c032ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce3c4cd-5808-4b9b-b7a9-b2ad15fe3dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nischalchand/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb65652d-1a47-4851-aa1a-1f56ab2cb50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.',\n",
       " 'If we were asked to\\nbuild such an application, think about how we would approach doing so at our organization.',\n",
       " 'We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.',\n",
       " 'Since language processing is involved, we would also list all the forms of text processing needed at each step.',\n",
       " 'This step-by-step processing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in building any NLP model.',\n",
       " 'These steps are common in every NLP project, so it\\nmakes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.',\n",
       " 'In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\\nabout when and how to use which step.',\n",
       " 'In later chapters, we‚Äôll discuss\\nspecific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "my_text = \"\"\"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. It is the series of steps involved in building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace. Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process. In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we‚Äôll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\"\"\"\n",
    "\n",
    "my_sentences = sent_tokenize(my_text)\n",
    "my_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890e1c88-29c0-4a7a-b56d-591d438142b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to\n",
      "build such an application, think about how we would approach doing so at our organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it\n",
      "makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
      "about when and how to use which step.\n",
      "['In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '‚Äô', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we‚Äôll discuss\n",
      "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\n",
      "['In', 'later', 'chapters', ',', 'we', '‚Äô', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4‚Äì7', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in my_sentences:\n",
    "    print(sentences)\n",
    "    print(word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd563f81-c8a5-405b-99d1-f2ae16f7671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'chapter',\n",
       " ',',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'some',\n",
       " 'common',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " 'that',\n",
       " 'we',\n",
       " 'might',\n",
       " 'encounter',\n",
       " 'in',\n",
       " 'everyday',\n",
       " 'life',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'were',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'build',\n",
       " 'such',\n",
       " 'an',\n",
       " 'application',\n",
       " ',',\n",
       " 'think',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'doing',\n",
       " 'so',\n",
       " 'at',\n",
       " 'our',\n",
       " 'organization',\n",
       " '.',\n",
       " 'We',\n",
       " 'would',\n",
       " 'normally',\n",
       " 'walk',\n",
       " 'through',\n",
       " 'the',\n",
       " 'requirements',\n",
       " 'and',\n",
       " 'break',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'down',\n",
       " 'into',\n",
       " 'several',\n",
       " 'sub-problems',\n",
       " ',',\n",
       " 'then',\n",
       " 'try',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'a',\n",
       " 'step-by-step',\n",
       " 'procedure',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'them',\n",
       " '.',\n",
       " 'Since',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'involved',\n",
       " ',',\n",
       " 'we',\n",
       " 'would',\n",
       " 'also',\n",
       " 'list',\n",
       " 'all',\n",
       " 'the',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'needed',\n",
       " 'at',\n",
       " 'each',\n",
       " 'step',\n",
       " '.',\n",
       " 'This',\n",
       " 'step-by-step',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'text',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'pipeline',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'the',\n",
       " 'series',\n",
       " 'of',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'building',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'model',\n",
       " '.',\n",
       " 'These',\n",
       " 'steps',\n",
       " 'are',\n",
       " 'common',\n",
       " 'in',\n",
       " 'every',\n",
       " 'NLP',\n",
       " 'project',\n",
       " ',',\n",
       " 'so',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'study',\n",
       " 'them',\n",
       " 'in',\n",
       " 'this',\n",
       " 'chapter',\n",
       " '.',\n",
       " 'Understanding',\n",
       " 'some',\n",
       " 'common',\n",
       " 'procedures',\n",
       " 'in',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'will',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'on',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'encountered',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplace',\n",
       " '.',\n",
       " 'Laying',\n",
       " 'out',\n",
       " 'and',\n",
       " 'developing',\n",
       " 'a',\n",
       " 'text-processing',\n",
       " 'pipeline',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'for',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'application',\n",
       " 'development',\n",
       " 'process',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'chapter',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'various',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'and',\n",
       " 'how',\n",
       " 'they',\n",
       " 'play',\n",
       " 'important',\n",
       " 'roles',\n",
       " 'in',\n",
       " 'solving',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'and',\n",
       " 'we',\n",
       " '‚Äô',\n",
       " 'll',\n",
       " 'see',\n",
       " 'a',\n",
       " 'few',\n",
       " 'guidelines',\n",
       " 'about',\n",
       " 'when',\n",
       " 'and',\n",
       " 'how',\n",
       " 'to',\n",
       " 'use',\n",
       " 'which',\n",
       " 'step',\n",
       " '.',\n",
       " 'In',\n",
       " 'later',\n",
       " 'chapters',\n",
       " ',',\n",
       " 'we',\n",
       " '‚Äô',\n",
       " 'll',\n",
       " 'discuss',\n",
       " 'specific',\n",
       " 'pipelines',\n",
       " 'for',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'Chapters',\n",
       " '4‚Äì7',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = word_tokenize(my_text)\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e3d3c6-1060-460c-a3a1-4a5c92864ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/nischalchand/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/nischalchand/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m----> 2\u001b[0m my_stopwords \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m my_stopwords\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/nischalchand/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "my_stopwords = stopwords.words('english')\n",
    "my_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36eb67-54ed-46d0-92f5-7636fd5a0107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
