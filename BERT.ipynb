{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9157be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ac1877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ec6b363e94473b9480770c44b03d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5f4f669ac04093841e0e1b123d35f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:15:55.150768: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-04-09 12:15:55.150837: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-04-09 12:15:55.150857: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-04-09 12:15:55.150907: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-09 12:15:55.150936: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6839e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2bdb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 101, 7592, 2088,  102,    0,    0],\n",
       "       [ 101, 7592, 2129, 2024, 2017,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(['hello world', 'hello how are you'], padding = True, truncation = True, return_tensors='tf')\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81d5d076-4dee-4ba3-b464-0b33fd4dd877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 6, 768), dtype=float32, numpy=\n",
       "array([[[-1.68883756e-01,  1.36063516e-01, -1.39400631e-01, ...,\n",
       "         -6.25112534e-01,  5.21728694e-02,  3.67145777e-01],\n",
       "        [-3.63274485e-01,  1.41218156e-01,  8.79987717e-01, ...,\n",
       "          1.04330614e-01,  2.88757414e-01,  3.72679204e-01],\n",
       "        [-6.98594093e-01, -6.98797822e-01,  6.45025298e-02, ...,\n",
       "         -2.21036404e-01,  9.86776501e-03, -5.93979418e-01],\n",
       "        [ 8.30982983e-01,  1.23666808e-01, -1.51189953e-01, ...,\n",
       "          1.03097424e-01, -6.77926600e-01, -2.62851775e-01],\n",
       "        [-4.02666271e-01, -1.92825329e-02,  5.73250055e-01, ...,\n",
       "         -2.06568196e-01,  2.33856291e-02,  2.01263204e-01],\n",
       "        [-6.22840822e-01, -2.74535656e-01,  1.81175709e-01, ...,\n",
       "         -1.29448786e-01, -3.83912474e-02, -5.73319346e-02]],\n",
       "\n",
       "       [[ 3.53770144e-02,  4.97097336e-03, -1.63990170e-01, ...,\n",
       "         -2.94695139e-01,  1.00460052e-01,  2.69069970e-01],\n",
       "        [ 1.91546619e-01,  1.55038625e-01,  3.97726029e-01, ...,\n",
       "         -6.07962370e-01,  5.23666561e-01, -2.53920019e-01],\n",
       "        [-3.58025968e-01, -6.16344750e-01,  4.82602194e-02, ...,\n",
       "         -2.88758427e-04,  7.10645020e-01, -6.09312236e-01],\n",
       "        [ 4.14312661e-01, -1.35979056e+00,  1.86189756e-01, ...,\n",
       "         -4.94022876e-01,  4.38691586e-01, -3.78174931e-01],\n",
       "        [-2.73466021e-01, -1.25411904e+00,  3.31949234e-01, ...,\n",
       "         -5.65506667e-02,  3.85584325e-01, -5.87253571e-01],\n",
       "        [ 5.24139225e-01, -2.56137848e-01, -3.20331782e-01, ...,\n",
       "          4.12313193e-01, -3.82963508e-01, -2.16114849e-01]]],\n",
       "      dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.9061535 , -0.31115326, -0.6216542 , ..., -0.3057524 ,\n",
       "        -0.6400939 ,  0.9166175 ],\n",
       "       [-0.9134232 , -0.32655174, -0.57403576, ..., -0.4289204 ,\n",
       "        -0.63983107,  0.9282712 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b67b9a-a2fc-4979-9662-d3f76ee54417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'I love \\xf0\\x9f\\x8d\\x95! shall we book a \\xf0\\x9f\\x9a\\x96 to gizza?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I love üçï! shall we book a üöñ to gizza?'\n",
    "text = text.encode(\"utf-8\")\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18781f84-64e3-4480-857f-d6c65a19b12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love üçï! shall we book a üöñ to gizza?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = text.decode('utf-8')\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9033c1a-1575-414d-8b02-594fbe755da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We notice that there are two errors in the output of the OCR system in this case.\\nDepending on the quality of the original scan, OCR output can potentially have\\nlarger amounts of errors. How do we clean up this text before feeding it into the next\\nstage of the pipeline? One approach is to run the text through a spell checker such as\\npyenchant [28], which will identify misspellings and suggest some alternatives. More\\nrecent approaches use neural network architectures to train word/character-based\\nlanguage models, which are in turn used for correcting OCR text output based on the\\ncontext [29].\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "filename = \"screenshot.png\"\n",
    "text = image_to_string(Image.open(filename))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74558b56-2a09-4694-ac30-b5ff82c032ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce3c4cd-5808-4b9b-b7a9-b2ad15fe3dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nischalchand/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb65652d-1a47-4851-aa1a-1f56ab2cb50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.',\n",
       " 'If we were asked to\\nbuild such an application, think about how we would approach doing so at our organization.',\n",
       " 'We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.',\n",
       " 'Since language processing is involved, we would also list all the forms of text processing needed at each step.',\n",
       " 'This step-by-step processing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in building any NLP model.',\n",
       " 'These steps are common in every NLP project, so it\\nmakes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.',\n",
       " 'In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\\nabout when and how to use which step.',\n",
       " 'In later chapters, we‚Äôll discuss\\nspecific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "my_text = \"\"\"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. It is the series of steps involved in building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace. Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process. In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we‚Äôll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\"\"\"\n",
    "\n",
    "my_sentences = sent_tokenize(my_text)\n",
    "my_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890e1c88-29c0-4a7a-b56d-591d438142b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to\n",
      "build such an application, think about how we would approach doing so at our organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it\n",
      "makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
      "about when and how to use which step.\n",
      "['In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '‚Äô', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we‚Äôll discuss\n",
      "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\n",
      "['In', 'later', 'chapters', ',', 'we', '‚Äô', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4‚Äì7', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in my_sentences:\n",
    "    print(sentences)\n",
    "    print(word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd563f81-c8a5-405b-99d1-f2ae16f7671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'chapter',\n",
       " ',',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'some',\n",
       " 'common',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " 'that',\n",
       " 'we',\n",
       " 'might',\n",
       " 'encounter',\n",
       " 'in',\n",
       " 'everyday',\n",
       " 'life',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'were',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'build',\n",
       " 'such',\n",
       " 'an',\n",
       " 'application',\n",
       " ',',\n",
       " 'think',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'doing',\n",
       " 'so',\n",
       " 'at',\n",
       " 'our',\n",
       " 'organization',\n",
       " '.',\n",
       " 'We',\n",
       " 'would',\n",
       " 'normally',\n",
       " 'walk',\n",
       " 'through',\n",
       " 'the',\n",
       " 'requirements',\n",
       " 'and',\n",
       " 'break',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'down',\n",
       " 'into',\n",
       " 'several',\n",
       " 'sub-problems',\n",
       " ',',\n",
       " 'then',\n",
       " 'try',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'a',\n",
       " 'step-by-step',\n",
       " 'procedure',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'them',\n",
       " '.',\n",
       " 'Since',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'involved',\n",
       " ',',\n",
       " 'we',\n",
       " 'would',\n",
       " 'also',\n",
       " 'list',\n",
       " 'all',\n",
       " 'the',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'needed',\n",
       " 'at',\n",
       " 'each',\n",
       " 'step',\n",
       " '.',\n",
       " 'This',\n",
       " 'step-by-step',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'text',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'pipeline',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'the',\n",
       " 'series',\n",
       " 'of',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'building',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'model',\n",
       " '.',\n",
       " 'These',\n",
       " 'steps',\n",
       " 'are',\n",
       " 'common',\n",
       " 'in',\n",
       " 'every',\n",
       " 'NLP',\n",
       " 'project',\n",
       " ',',\n",
       " 'so',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'study',\n",
       " 'them',\n",
       " 'in',\n",
       " 'this',\n",
       " 'chapter',\n",
       " '.',\n",
       " 'Understanding',\n",
       " 'some',\n",
       " 'common',\n",
       " 'procedures',\n",
       " 'in',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'will',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'on',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'encountered',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplace',\n",
       " '.',\n",
       " 'Laying',\n",
       " 'out',\n",
       " 'and',\n",
       " 'developing',\n",
       " 'a',\n",
       " 'text-processing',\n",
       " 'pipeline',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'for',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'application',\n",
       " 'development',\n",
       " 'process',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'chapter',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'various',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'and',\n",
       " 'how',\n",
       " 'they',\n",
       " 'play',\n",
       " 'important',\n",
       " 'roles',\n",
       " 'in',\n",
       " 'solving',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'and',\n",
       " 'we',\n",
       " '‚Äô',\n",
       " 'll',\n",
       " 'see',\n",
       " 'a',\n",
       " 'few',\n",
       " 'guidelines',\n",
       " 'about',\n",
       " 'when',\n",
       " 'and',\n",
       " 'how',\n",
       " 'to',\n",
       " 'use',\n",
       " 'which',\n",
       " 'step',\n",
       " '.',\n",
       " 'In',\n",
       " 'later',\n",
       " 'chapters',\n",
       " ',',\n",
       " 'we',\n",
       " '‚Äô',\n",
       " 'll',\n",
       " 'discuss',\n",
       " 'specific',\n",
       " 'pipelines',\n",
       " 'for',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'Chapters',\n",
       " '4‚Äì7',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = word_tokenize(my_text)\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e3d3c6-1060-460c-a3a1-4a5c92864ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['previous',\n",
       " 'chapter',\n",
       " ',',\n",
       " 'saw',\n",
       " 'examples',\n",
       " 'common',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " 'might',\n",
       " 'encounter',\n",
       " 'everyday',\n",
       " 'life',\n",
       " '.',\n",
       " 'asked',\n",
       " 'build',\n",
       " 'application',\n",
       " ',',\n",
       " 'think',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'organization',\n",
       " '.',\n",
       " 'would',\n",
       " 'normally',\n",
       " 'walk',\n",
       " 'requirements',\n",
       " 'break',\n",
       " 'problem',\n",
       " 'several',\n",
       " 'sub-problems',\n",
       " ',',\n",
       " 'try',\n",
       " 'develop',\n",
       " 'step-by-step',\n",
       " 'procedure',\n",
       " 'solve',\n",
       " '.',\n",
       " 'Since',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'involved',\n",
       " ',',\n",
       " 'would',\n",
       " 'also',\n",
       " 'list',\n",
       " 'forms',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'needed',\n",
       " 'step',\n",
       " '.',\n",
       " 'step-by-step',\n",
       " 'processing',\n",
       " 'text',\n",
       " 'known',\n",
       " 'pipeline',\n",
       " '.',\n",
       " 'series',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'building',\n",
       " 'NLP',\n",
       " 'model',\n",
       " '.',\n",
       " 'steps',\n",
       " 'common',\n",
       " 'every',\n",
       " 'NLP',\n",
       " 'project',\n",
       " ',',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'study',\n",
       " 'chapter',\n",
       " '.',\n",
       " 'Understanding',\n",
       " 'common',\n",
       " 'procedures',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'get',\n",
       " 'started',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'encountered',\n",
       " 'workplace',\n",
       " '.',\n",
       " 'Laying',\n",
       " 'developing',\n",
       " 'text-processing',\n",
       " 'pipeline',\n",
       " 'seen',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'NLP',\n",
       " 'application',\n",
       " 'development',\n",
       " 'process',\n",
       " '.',\n",
       " 'chapter',\n",
       " ',',\n",
       " 'learn',\n",
       " 'various',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'play',\n",
       " 'important',\n",
       " 'roles',\n",
       " 'solving',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " '‚Äô',\n",
       " 'see',\n",
       " 'guidelines',\n",
       " 'use',\n",
       " 'step',\n",
       " '.',\n",
       " 'later',\n",
       " 'chapters',\n",
       " ',',\n",
       " '‚Äô',\n",
       " 'discuss',\n",
       " 'specific',\n",
       " 'pipelines',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'Chapters',\n",
       " '4‚Äì7',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "removed_words = []\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in my_words if not w.lower() in my_stopwords]\n",
    "\n",
    "filtered_sentence\n",
    " \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa36eb67-54ed-46d0-92f5-7636fd5a0107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nischalchand/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cb8f822-63ac-4074-9736-523a0fb19445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559b5acf-3cf9-4143-b58f-9b9dc6cedd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '``',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-./',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "token_puntuation = word_tokenize(punctuation)\n",
    "token_puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af89b05-2891-423f-b9c6-442a790aab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = []\n",
    "for sentence in filtered_sentence:\n",
    "    if sentence not in token_puntuation:\n",
    "        cleaned_list.append(sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8362134e-ec7b-499e-8abe-6c1001470ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['previous',\n",
       " 'chapter',\n",
       " 'saw',\n",
       " 'examples',\n",
       " 'common',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " 'might',\n",
       " 'encounter',\n",
       " 'everyday',\n",
       " 'life',\n",
       " '.',\n",
       " 'asked',\n",
       " 'build',\n",
       " 'application',\n",
       " 'think',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'organization',\n",
       " '.',\n",
       " 'would',\n",
       " 'normally',\n",
       " 'walk',\n",
       " 'requirements',\n",
       " 'break',\n",
       " 'problem',\n",
       " 'several',\n",
       " 'sub-problems',\n",
       " 'try',\n",
       " 'develop',\n",
       " 'step-by-step',\n",
       " 'procedure',\n",
       " 'solve',\n",
       " '.',\n",
       " 'Since',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'involved',\n",
       " 'would',\n",
       " 'also',\n",
       " 'list',\n",
       " 'forms',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'needed',\n",
       " 'step',\n",
       " '.',\n",
       " 'step-by-step',\n",
       " 'processing',\n",
       " 'text',\n",
       " 'known',\n",
       " 'pipeline',\n",
       " '.',\n",
       " 'series',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'building',\n",
       " 'NLP',\n",
       " 'model',\n",
       " '.',\n",
       " 'steps',\n",
       " 'common',\n",
       " 'every',\n",
       " 'NLP',\n",
       " 'project',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'study',\n",
       " 'chapter',\n",
       " '.',\n",
       " 'Understanding',\n",
       " 'common',\n",
       " 'procedures',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'get',\n",
       " 'started',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'encountered',\n",
       " 'workplace',\n",
       " '.',\n",
       " 'Laying',\n",
       " 'developing',\n",
       " 'text-processing',\n",
       " 'pipeline',\n",
       " 'seen',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'NLP',\n",
       " 'application',\n",
       " 'development',\n",
       " 'process',\n",
       " '.',\n",
       " 'chapter',\n",
       " 'learn',\n",
       " 'various',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'play',\n",
       " 'important',\n",
       " 'roles',\n",
       " 'solving',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " '‚Äô',\n",
       " 'see',\n",
       " 'guidelines',\n",
       " 'use',\n",
       " 'step',\n",
       " '.',\n",
       " 'later',\n",
       " 'chapters',\n",
       " '‚Äô',\n",
       " 'discuss',\n",
       " 'specific',\n",
       " 'pipelines',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'e.g.',\n",
       " 'Chapters',\n",
       " '4‚Äì7',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e3714e-bfd0-4b36-bf5a-3235ff740278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece40822-8e6d-494b-bee4-f274548d78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a57a41ac-044d-48b4-a377-91202c527507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['previou',\n",
       " 'chapter',\n",
       " 'saw',\n",
       " 'exampl',\n",
       " 'common',\n",
       " 'nlp',\n",
       " 'applic',\n",
       " 'might',\n",
       " 'encount',\n",
       " 'everyday',\n",
       " 'life',\n",
       " '.',\n",
       " 'ask',\n",
       " 'build',\n",
       " 'applic',\n",
       " 'think',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'organ',\n",
       " '.',\n",
       " 'would',\n",
       " 'normal',\n",
       " 'walk',\n",
       " 'requir',\n",
       " 'break',\n",
       " 'problem',\n",
       " 'sever',\n",
       " 'sub-problem',\n",
       " 'tri',\n",
       " 'develop',\n",
       " 'step-by-step',\n",
       " 'procedur',\n",
       " 'solv',\n",
       " '.',\n",
       " 'sinc',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'involv',\n",
       " 'would',\n",
       " 'also',\n",
       " 'list',\n",
       " 'form',\n",
       " 'text',\n",
       " 'process',\n",
       " 'need',\n",
       " 'step',\n",
       " '.',\n",
       " 'step-by-step',\n",
       " 'process',\n",
       " 'text',\n",
       " 'known',\n",
       " 'pipelin',\n",
       " '.',\n",
       " 'seri',\n",
       " 'step',\n",
       " 'involv',\n",
       " 'build',\n",
       " 'nlp',\n",
       " 'model',\n",
       " '.',\n",
       " 'step',\n",
       " 'common',\n",
       " 'everi',\n",
       " 'nlp',\n",
       " 'project',\n",
       " 'make',\n",
       " 'sens',\n",
       " 'studi',\n",
       " 'chapter',\n",
       " '.',\n",
       " 'understand',\n",
       " 'common',\n",
       " 'procedur',\n",
       " 'nlp',\n",
       " 'pipelin',\n",
       " 'enabl',\n",
       " 'us',\n",
       " 'get',\n",
       " 'start',\n",
       " 'nlp',\n",
       " 'problem',\n",
       " 'encount',\n",
       " 'workplac',\n",
       " '.',\n",
       " 'lay',\n",
       " 'develop',\n",
       " 'text-process',\n",
       " 'pipelin',\n",
       " 'seen',\n",
       " 'start',\n",
       " 'point',\n",
       " 'nlp',\n",
       " 'applic',\n",
       " 'develop',\n",
       " 'process',\n",
       " '.',\n",
       " 'chapter',\n",
       " 'learn',\n",
       " 'variou',\n",
       " 'step',\n",
       " 'involv',\n",
       " 'play',\n",
       " 'import',\n",
       " 'role',\n",
       " 'solv',\n",
       " 'nlp',\n",
       " 'problem',\n",
       " '‚Äô',\n",
       " 'see',\n",
       " 'guidelin',\n",
       " 'use',\n",
       " 'step',\n",
       " '.',\n",
       " 'later',\n",
       " 'chapter',\n",
       " '‚Äô',\n",
       " 'discuss',\n",
       " 'specif',\n",
       " 'pipelin',\n",
       " 'variou',\n",
       " 'nlp',\n",
       " 'task',\n",
       " 'e.g.',\n",
       " 'chapter',\n",
       " '4‚Äì7',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_list = [stemmer.stem(word) for word in cleaned_list]\n",
    "stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8dbc55-ba61-457d-8890-19918fddcd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previou', 'chapter', 'saw', 'exampl', 'common', 'nlp', 'applic', 'might', 'encount', 'everyday', 'life', 'ask', 'build', 'applic', 'think', 'would', 'approach', 'organ', 'would', 'normal', 'walk', 'requir', 'break', 'problem', 'sever', 'sub-problem', 'tri', 'develop', 'step-by-step', 'procedur', 'solv', 'sinc', 'languag', 'process', 'involv', 'would', 'also', 'list', 'form', 'text', 'process', 'need', 'step', 'step-by-step', 'process', 'text', 'known', 'pipelin', 'seri', 'step', 'involv', 'build', 'nlp', 'model', 'step', 'common', 'everi', 'nlp', 'project', 'make', 'sens', 'studi', 'chapter', 'understand', 'common', 'procedur', 'nlp', 'pipelin', 'enabl', 'us', 'get', 'start', 'nlp', 'problem', 'encount', 'workplac', 'lay', 'develop', 'text-process', 'pipelin', 'seen', 'start', 'point', 'nlp', 'applic', 'develop', 'process', 'chapter', 'learn', 'variou', 'step', 'involv', 'play', 'import', 'role', 'solv', 'nlp', 'problem', '‚Äô', 'see', 'guidelin', 'use', 'step', 'later', 'chapter', '‚Äô', 'discuss', 'specif', 'pipelin', 'variou', 'nlp', 'task', 'e.g.', 'chapter', '4‚Äì7']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(stemmed_list):\n",
    "    return [elem for elem in stemmed_list if elem not in string.punctuation]\n",
    "\n",
    "\n",
    "cleaned_list = remove_punctuation(stemmed_list)\n",
    "print(cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146c6c54-79e4-45c9-aa87-e4da37a984c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talking talk\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "token = sp(u'talking')\n",
    "for word in token:\n",
    "    print( word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9796b84-68f2-4fde-ab63-0a2f2ed19164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Downloading spacy-3.7.4-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m989.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m488.4/488.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m781.1/781.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78840499-7545-40f5-8bed-94e715586107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"\"\"Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr\"\"\")\n",
    "doc\n",
    " # for token in doc:\n",
    "    \n",
    "#     print(token.text, token.lemma_, token.pos_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76217d7b-b1e2-4f18-a93e-e9c89d5508a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles PROPN Charles\n",
      "Spencer PROPN Spencer\n",
      "Chaplin PROPN Chaplin\n",
      "was AUX be\n",
      "born VERB bear\n",
      "on ADP on\n",
      "16 NUM 16\n",
      "April PROPN April\n",
      "1889 NUM 1889\n",
      "toHannah PROPN toHannah\n",
      "Chaplin PROPN Chaplin\n",
      "( PUNCT (\n",
      "born VERB bear\n",
      "Hannah PROPN Hannah\n",
      "Harriet PROPN Harriet\n",
      "Pedlingham PROPN Pedlingham\n",
      "Hill PROPN Hill\n",
      ") PUNCT )\n",
      "and CCONJ and\n",
      "Charles PROPN Charles\n",
      "Chaplin PROPN Chaplin\n",
      "Sr PROPN Sr\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "sp = spacy.load('en_core_web_sm')\n",
    "sentence = 'Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr'\n",
    "doc = sp(sentence)\n",
    "for token in doc:\n",
    "    print(token.t, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f19e6ac7-782c-4771-8820-c790a9a62a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('students.txt') as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71646ffc-9c91-40a7-acb9-b0cc805cba1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = \" \".join(text)\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b863fc1-b50f-48c7-800b-dde987d21830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = sp(strings)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18238417-bbc4-4672-9477-c4f5f5e77f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.Strange loves Nepal.\n",
      "He even has one of his movie in nepal\n"
     ]
    }
   ],
   "source": [
    "text = 'Dr.Strange loves Nepal. He even has one of his movie in nepal'\n",
    "doc = sp(text)\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ac5dc6-cb4e-4ac7-a08c-f50770c6e1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc = sp(text)\n",
    "url = [token.text for token in doc if token.like_url]\n",
    "\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec2560e7-aa54-462f-a389-db5fe76f13bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 ‚Ç¨\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 ‚Ç¨ to Steve\"\n",
    "\n",
    "doc = sp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i + 1].is_currency:\n",
    "        print(token.text, doc[token.i+1].text)\n",
    "        \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c43070-da2c-4d64-80a8-1b1e9f43b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "doc = sp(' I own Microsoft my annual income is 100,00$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b04b0f-2316-4fa5-938f-39396cc2ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft ORG\n",
      "annual DATE\n",
      "100,00$ MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c14e6e-c2f2-4ec9-ae15-0b9bc804e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e48d51fb-cc86-40fa-8e1d-9264e0d73ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> I own \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " my \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    annual\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " income is \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100,00$\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd53e1a5-049a-48f0-bad7-d553144b1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c9133f0-1074-49e2-a2d2-85af528c8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy customization\n",
    "\n",
    "\n",
    "ar = sp.get_pipe('attribute_ruler')\n",
    "ar.add([[{\"TEXT\":\"bro\"}], [{\"TEXT\":\"brah\"}]], {\"LEMMA\": \"Brother\"})\n",
    "text = \"bro, i am really exhausted brah, can't we not go?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "151bd5ee-a474-48fb-9901-7eff5d54fcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bro | Brother\n",
      ", | ,\n",
      "i | I\n",
      "am | be\n",
      "really | really\n",
      "exhausted | exhausted\n",
      "brah | Brother\n",
      ", | ,\n",
      "ca | can\n",
      "n't | not\n",
      "we | we\n",
      "not | not\n",
      "go | go\n",
      "? | ?\n"
     ]
    }
   ],
   "source": [
    "doc = sp(text)\n",
    "for token in doc:\n",
    "    print(token.text, '|', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21c0349a-adf5-4f6c-b785-edddad6d6d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | proper noun | NNP | noun, proper singular\n",
      "and | CCONJ | coordinating conjunction | CC | conjunction, coordinating\n",
      "Dr. | PROPN | proper noun | NNP | noun, proper singular\n",
      "Strange | PROPN | proper noun | NNP | noun, proper singular\n",
      "flew | VERB | verb | VBD | verb, past tense\n",
      "to | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "mars | NOUN | noun | NNS | noun, plural\n",
      "yesterday | NOUN | noun | NN | noun, singular or mass\n",
      ". | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "He | PRON | pronoun | PRP | pronoun, personal\n",
      "carried | VERB | verb | VBD | verb, past tense\n",
      "biryani | ADJ | adjective | JJ | adjective (English), other noun-modifier (Chinese)\n",
      "masala | NOUN | noun | NN | noun, singular or mass\n",
      "with | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "him | PRON | pronoun | PRP | pronoun, personal\n"
     ]
    }
   ],
   "source": [
    "doc = sp('Elon and Dr.Strange flew to mars yesterday. He carried biryani masala with him')\n",
    "for token in doc:\n",
    "    print(token.text, '|', token.pos_, '|', spacy.explain(token.pos_), '|', token.tag_, '|', spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28a08721-0c1f-42d0-9cfe-ccc3ad78d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"\"\"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. It is the series of steps involved in building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace. Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process. In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we‚Äôll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43270fdc-ca70-4925-932f-693edd6ecdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = sp(my_text)\n",
    "filtered_token = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        filtered_token.append(token.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edd7644d-41d9-45a7-9d9c-9e317b6cfea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'chapter',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'some',\n",
       " 'common',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " 'that',\n",
       " 'we',\n",
       " 'might',\n",
       " 'encounter',\n",
       " 'in',\n",
       " 'everyday',\n",
       " 'life',\n",
       " 'If',\n",
       " 'we',\n",
       " 'were',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'build',\n",
       " 'such',\n",
       " 'an',\n",
       " 'application',\n",
       " 'think',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'doing',\n",
       " 'so',\n",
       " 'at',\n",
       " 'our',\n",
       " 'organization',\n",
       " 'We',\n",
       " 'would',\n",
       " 'normally',\n",
       " 'walk',\n",
       " 'through',\n",
       " 'the',\n",
       " 'requirements',\n",
       " 'and',\n",
       " 'break',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'down',\n",
       " 'into',\n",
       " 'several',\n",
       " 'sub',\n",
       " '-',\n",
       " 'problems',\n",
       " 'then',\n",
       " 'try',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'a',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'procedure',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'them',\n",
       " 'Since',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'involved',\n",
       " 'we',\n",
       " 'would',\n",
       " 'also',\n",
       " 'list',\n",
       " 'all',\n",
       " 'the',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'needed',\n",
       " 'at',\n",
       " 'each',\n",
       " 'step',\n",
       " 'This',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'text',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'pipeline',\n",
       " 'It',\n",
       " 'is',\n",
       " 'the',\n",
       " 'series',\n",
       " 'of',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'building',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'model',\n",
       " 'These',\n",
       " 'steps',\n",
       " 'are',\n",
       " 'common',\n",
       " 'in',\n",
       " 'every',\n",
       " 'NLP',\n",
       " 'project',\n",
       " 'so',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'study',\n",
       " 'them',\n",
       " 'in',\n",
       " 'this',\n",
       " 'chapter',\n",
       " 'Understanding',\n",
       " 'some',\n",
       " 'common',\n",
       " 'procedures',\n",
       " 'in',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'will',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'on',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'encountered',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplace',\n",
       " 'Laying',\n",
       " 'out',\n",
       " 'and',\n",
       " 'developing',\n",
       " 'a',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'pipeline',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'for',\n",
       " 'any',\n",
       " 'NLP',\n",
       " 'application',\n",
       " 'development',\n",
       " 'process',\n",
       " 'In',\n",
       " 'this',\n",
       " 'chapter',\n",
       " 'we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'various',\n",
       " 'steps',\n",
       " 'involved',\n",
       " 'and',\n",
       " 'how',\n",
       " 'they',\n",
       " 'play',\n",
       " 'important',\n",
       " 'roles',\n",
       " 'in',\n",
       " 'solving',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'and',\n",
       " 'we',\n",
       " '‚Äôll',\n",
       " 'see',\n",
       " 'a',\n",
       " 'few',\n",
       " 'guidelines',\n",
       " 'about',\n",
       " 'when',\n",
       " 'and',\n",
       " 'how',\n",
       " 'to',\n",
       " 'use',\n",
       " 'which',\n",
       " 'step',\n",
       " 'In',\n",
       " 'later',\n",
       " 'chapters',\n",
       " 'we',\n",
       " '‚Äôll',\n",
       " 'discuss',\n",
       " 'specific',\n",
       " 'pipelines',\n",
       " 'for',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'e.g.',\n",
       " 'Chapters',\n",
       " '4‚Äì7']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7364bbd7-3822-4b67-9f8c-ed5d1bd29fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('news_story.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36945ac6-dc8f-4113-8030-0968ed8391de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\\n\\nThe consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from March‚Äôs peak but was still close to the highest level since the summer of 1982.\\n\\nRemoving volatile food and energy prices, so-called core CPI still rose 6.2%, against expectations for a 6% gain, clouding hopes that inflation had peaked in March.\\n\\nThe month-over-month gains also were higher than expectations ‚Äî 0.3% on headline CPI versus the 0.2% estimate and a 0.6% increase for core, against the outlook for a 0.4% gain.\\n\\nThe price gains also meant that workers continued to lose ground. Real wages adjusted for inflation decreased 0.1% on the month despite a nominal increase of 0.3% in average hourly earnings. Over the past year, real earnings have dropped 2.6% even though average hourly earnings are up 5.5%.\\n\\nInflation has been the single biggest threat to a recovery that began early in the Covid pandemic and saw the economy in 2021 stage its biggest single-year growth level since 1984. Rising prices at the pump and in grocery stores have been one problem, but inflation has spread beyond those two areas into housing, auto sales and a host of other areas.\\n\\nFederal Reserve officials have responded to the problem with two interest rate hikes so far this year and pledges of more until inflation comes down to the central bank‚Äôs 2% goal. However, Wednesday‚Äôs data shows that the Fed has a big job ahead.\\n\\nCredits: cnbc.com'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6d0ac00-6616-4ee2-a356-474c45fd1010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation | NOUN\n",
      "rose | VERB\n",
      "again | ADV\n",
      "in | ADP\n",
      "April | PROPN\n",
      ", | PUNCT\n",
      "continuing | VERB\n",
      "a | DET\n",
      "climb | NOUN\n",
      "that | PRON\n",
      "has | AUX\n",
      "pushed | VERB\n",
      "consumers | NOUN\n",
      "to | ADP\n",
      "the | DET\n",
      "brink | NOUN\n",
      "and | CCONJ\n",
      "is | AUX\n",
      "threatening | VERB\n",
      "the | DET\n",
      "economic | ADJ\n",
      "expansion | NOUN\n",
      ", | PUNCT\n",
      "the | DET\n",
      "Bureau | PROPN\n",
      "of | ADP\n",
      "Labor | PROPN\n",
      "Statistics | PROPN\n",
      "reported | VERB\n",
      "Wednesday | PROPN\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "The | DET\n",
      "consumer | NOUN\n",
      "price | NOUN\n",
      "index | NOUN\n",
      ", | PUNCT\n",
      "a | DET\n",
      "broad | ADV\n",
      "- | PUNCT\n",
      "based | VERB\n",
      "measure | NOUN\n",
      "of | ADP\n",
      "prices | NOUN\n",
      "for | ADP\n",
      "goods | NOUN\n",
      "and | CCONJ\n",
      "services | NOUN\n",
      ", | PUNCT\n",
      "increased | VERB\n",
      "8.3 | NUM\n",
      "% | NOUN\n",
      "from | ADP\n",
      "a | DET\n",
      "year | NOUN\n",
      "ago | ADV\n",
      ", | PUNCT\n",
      "higher | ADJ\n",
      "than | ADP\n",
      "the | DET\n",
      "Dow | PROPN\n",
      "Jones | PROPN\n",
      "estimate | NOUN\n",
      "for | ADP\n",
      "an | DET\n",
      "8.1 | NUM\n",
      "% | NOUN\n",
      "gain | NOUN\n",
      ". | PUNCT\n",
      "That | PRON\n",
      "represented | VERB\n",
      "a | DET\n",
      "slight | ADJ\n",
      "ease | NOUN\n",
      "from | ADP\n",
      "March | PROPN\n",
      "‚Äôs | PART\n",
      "peak | NOUN\n",
      "but | CCONJ\n",
      "was | AUX\n",
      "still | ADV\n",
      "close | ADJ\n",
      "to | ADP\n",
      "the | DET\n",
      "highest | ADJ\n",
      "level | NOUN\n",
      "since | SCONJ\n",
      "the | DET\n",
      "summer | NOUN\n",
      "of | ADP\n",
      "1982 | NUM\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "Removing | VERB\n",
      "volatile | ADJ\n",
      "food | NOUN\n",
      "and | CCONJ\n",
      "energy | NOUN\n",
      "prices | NOUN\n",
      ", | PUNCT\n",
      "so | ADV\n",
      "- | PUNCT\n",
      "called | VERB\n",
      "core | NOUN\n",
      "CPI | PROPN\n",
      "still | ADV\n",
      "rose | VERB\n",
      "6.2 | NUM\n",
      "% | NOUN\n",
      ", | PUNCT\n",
      "against | ADP\n",
      "expectations | NOUN\n",
      "for | ADP\n",
      "a | DET\n",
      "6 | NUM\n",
      "% | NOUN\n",
      "gain | NOUN\n",
      ", | PUNCT\n",
      "clouding | VERB\n",
      "hopes | NOUN\n",
      "that | SCONJ\n",
      "inflation | NOUN\n",
      "had | AUX\n",
      "peaked | VERB\n",
      "in | ADP\n",
      "March | PROPN\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "The | DET\n",
      "month | NOUN\n",
      "- | PUNCT\n",
      "over | ADP\n",
      "- | PUNCT\n",
      "month | NOUN\n",
      "gains | NOUN\n",
      "also | ADV\n",
      "were | AUX\n",
      "higher | ADJ\n",
      "than | ADP\n",
      "expectations | NOUN\n",
      "‚Äî | PUNCT\n",
      "0.3 | NUM\n",
      "% | NOUN\n",
      "on | ADP\n",
      "headline | NOUN\n",
      "CPI | PROPN\n",
      "versus | ADP\n",
      "the | DET\n",
      "0.2 | NUM\n",
      "% | NOUN\n",
      "estimate | NOUN\n",
      "and | CCONJ\n",
      "a | DET\n",
      "0.6 | NUM\n",
      "% | NOUN\n",
      "increase | NOUN\n",
      "for | ADP\n",
      "core | NOUN\n",
      ", | PUNCT\n",
      "against | ADP\n",
      "the | DET\n",
      "outlook | NOUN\n",
      "for | ADP\n",
      "a | DET\n",
      "0.4 | NUM\n",
      "% | NOUN\n",
      "gain | NOUN\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "The | DET\n",
      "price | NOUN\n",
      "gains | NOUN\n",
      "also | ADV\n",
      "meant | VERB\n",
      "that | SCONJ\n",
      "workers | NOUN\n",
      "continued | VERB\n",
      "to | PART\n",
      "lose | VERB\n",
      "ground | NOUN\n",
      ". | PUNCT\n",
      "Real | ADJ\n",
      "wages | NOUN\n",
      "adjusted | VERB\n",
      "for | ADP\n",
      "inflation | NOUN\n",
      "decreased | VERB\n",
      "0.1 | NUM\n",
      "% | NOUN\n",
      "on | ADP\n",
      "the | DET\n",
      "month | NOUN\n",
      "despite | SCONJ\n",
      "a | DET\n",
      "nominal | ADJ\n",
      "increase | NOUN\n",
      "of | ADP\n",
      "0.3 | NUM\n",
      "% | NOUN\n",
      "in | ADP\n",
      "average | ADJ\n",
      "hourly | ADJ\n",
      "earnings | NOUN\n",
      ". | PUNCT\n",
      "Over | ADP\n",
      "the | DET\n",
      "past | ADJ\n",
      "year | NOUN\n",
      ", | PUNCT\n",
      "real | ADJ\n",
      "earnings | NOUN\n",
      "have | AUX\n",
      "dropped | VERB\n",
      "2.6 | NUM\n",
      "% | NOUN\n",
      "even | ADV\n",
      "though | SCONJ\n",
      "average | ADJ\n",
      "hourly | ADJ\n",
      "earnings | NOUN\n",
      "are | AUX\n",
      "up | ADV\n",
      "5.5 | NUM\n",
      "% | NOUN\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "Inflation | NOUN\n",
      "has | AUX\n",
      "been | AUX\n",
      "the | DET\n",
      "single | ADJ\n",
      "biggest | ADJ\n",
      "threat | NOUN\n",
      "to | ADP\n",
      "a | DET\n",
      "recovery | NOUN\n",
      "that | PRON\n",
      "began | VERB\n",
      "early | ADV\n",
      "in | ADP\n",
      "the | DET\n",
      "Covid | PROPN\n",
      "pandemic | NOUN\n",
      "and | CCONJ\n",
      "saw | VERB\n",
      "the | DET\n",
      "economy | NOUN\n",
      "in | ADP\n",
      "2021 | NUM\n",
      "stage | NOUN\n",
      "its | PRON\n",
      "biggest | ADJ\n",
      "single | ADJ\n",
      "- | PUNCT\n",
      "year | NOUN\n",
      "growth | NOUN\n",
      "level | NOUN\n",
      "since | SCONJ\n",
      "1984 | NUM\n",
      ". | PUNCT\n",
      "Rising | VERB\n",
      "prices | NOUN\n",
      "at | ADP\n",
      "the | DET\n",
      "pump | NOUN\n",
      "and | CCONJ\n",
      "in | ADP\n",
      "grocery | NOUN\n",
      "stores | NOUN\n",
      "have | AUX\n",
      "been | AUX\n",
      "one | NUM\n",
      "problem | NOUN\n",
      ", | PUNCT\n",
      "but | CCONJ\n",
      "inflation | NOUN\n",
      "has | AUX\n",
      "spread | VERB\n",
      "beyond | ADP\n",
      "those | DET\n",
      "two | NUM\n",
      "areas | NOUN\n",
      "into | ADP\n",
      "housing | NOUN\n",
      ", | PUNCT\n",
      "auto | NOUN\n",
      "sales | NOUN\n",
      "and | CCONJ\n",
      "a | DET\n",
      "host | NOUN\n",
      "of | ADP\n",
      "other | ADJ\n",
      "areas | NOUN\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "Federal | PROPN\n",
      "Reserve | PROPN\n",
      "officials | NOUN\n",
      "have | AUX\n",
      "responded | VERB\n",
      "to | ADP\n",
      "the | DET\n",
      "problem | NOUN\n",
      "with | ADP\n",
      "two | NUM\n",
      "interest | NOUN\n",
      "rate | NOUN\n",
      "hikes | NOUN\n",
      "so | ADV\n",
      "far | ADV\n",
      "this | DET\n",
      "year | NOUN\n",
      "and | CCONJ\n",
      "pledges | NOUN\n",
      "of | ADP\n",
      "more | ADJ\n",
      "until | SCONJ\n",
      "inflation | NOUN\n",
      "comes | VERB\n",
      "down | ADP\n",
      "to | ADP\n",
      "the | DET\n",
      "central | ADJ\n",
      "bank | NOUN\n",
      "‚Äôs | PART\n",
      "2 | NUM\n",
      "% | NOUN\n",
      "goal | NOUN\n",
      ". | PUNCT\n",
      "However | ADV\n",
      ", | PUNCT\n",
      "Wednesday | PROPN\n",
      "‚Äôs | PART\n",
      "data | NOUN\n",
      "shows | VERB\n",
      "that | SCONJ\n",
      "the | DET\n",
      "Fed | PROPN\n",
      "has | VERB\n",
      "a | DET\n",
      "big | ADJ\n",
      "job | NOUN\n",
      "ahead | ADV\n",
      ". | PUNCT\n",
      "\n",
      "\n",
      " | SPACE\n",
      "Credits | NOUN\n",
      ": | PUNCT\n",
      "cnbc.com | X\n"
     ]
    }
   ],
   "source": [
    "doc = sp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, '|', token.pos_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e23552c-aa0e-4a0a-bd71-b59c059cd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = []\n",
    "num_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        noun_list.append(token.text)\n",
    "    if token.pos_ == \"NUM\":\n",
    "        num_list.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4a9491b-c5c9-46d7-b5f9-2cac4c6e785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inflation',\n",
       " 'climb',\n",
       " 'consumers',\n",
       " 'brink',\n",
       " 'expansion',\n",
       " 'consumer',\n",
       " 'price',\n",
       " 'index',\n",
       " 'measure',\n",
       " 'prices',\n",
       " 'goods',\n",
       " 'services',\n",
       " '%',\n",
       " 'year',\n",
       " 'estimate',\n",
       " '%',\n",
       " 'gain',\n",
       " 'ease',\n",
       " 'peak',\n",
       " 'level',\n",
       " 'summer',\n",
       " 'food',\n",
       " 'energy',\n",
       " 'prices',\n",
       " 'core',\n",
       " '%',\n",
       " 'expectations',\n",
       " '%',\n",
       " 'gain',\n",
       " 'hopes',\n",
       " 'inflation',\n",
       " 'month',\n",
       " 'month',\n",
       " 'gains',\n",
       " 'expectations',\n",
       " '%',\n",
       " 'headline',\n",
       " '%',\n",
       " 'estimate',\n",
       " '%',\n",
       " 'increase',\n",
       " 'core',\n",
       " 'outlook',\n",
       " '%',\n",
       " 'gain',\n",
       " 'price',\n",
       " 'gains',\n",
       " 'workers',\n",
       " 'ground',\n",
       " 'wages',\n",
       " 'inflation',\n",
       " '%',\n",
       " 'month',\n",
       " 'increase',\n",
       " '%',\n",
       " 'earnings',\n",
       " 'year',\n",
       " 'earnings',\n",
       " '%',\n",
       " 'earnings',\n",
       " '%',\n",
       " 'Inflation',\n",
       " 'threat',\n",
       " 'recovery',\n",
       " 'pandemic',\n",
       " 'economy',\n",
       " 'stage',\n",
       " 'year',\n",
       " 'growth',\n",
       " 'level',\n",
       " 'prices',\n",
       " 'pump',\n",
       " 'grocery',\n",
       " 'stores',\n",
       " 'problem',\n",
       " 'inflation',\n",
       " 'areas',\n",
       " 'housing',\n",
       " 'auto',\n",
       " 'sales',\n",
       " 'host',\n",
       " 'areas',\n",
       " 'officials',\n",
       " 'problem',\n",
       " 'interest',\n",
       " 'rate',\n",
       " 'hikes',\n",
       " 'year',\n",
       " 'pledges',\n",
       " 'inflation',\n",
       " 'bank',\n",
       " '%',\n",
       " 'goal',\n",
       " 'data',\n",
       " 'job',\n",
       " 'Credits']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "480436f1-e366-42d1-a4b7-3e8005dd2efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8.3',\n",
       " '8.1',\n",
       " '1982',\n",
       " '6.2',\n",
       " '6',\n",
       " '0.3',\n",
       " '0.2',\n",
       " '0.6',\n",
       " '0.4',\n",
       " '0.1',\n",
       " '0.3',\n",
       " '2.6',\n",
       " '5.5',\n",
       " '2021',\n",
       " '1984',\n",
       " 'one',\n",
       " 'two',\n",
       " 'two',\n",
       " '2']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd9a7463-22fe-407e-8b19-5fd1a13cbe9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 96,\n",
       " 100: 27,\n",
       " 86: 15,\n",
       " 85: 39,\n",
       " 96: 16,\n",
       " 97: 32,\n",
       " 90: 34,\n",
       " 95: 4,\n",
       " 87: 13,\n",
       " 89: 10,\n",
       " 84: 23,\n",
       " 103: 7,\n",
       " 93: 19,\n",
       " 94: 4,\n",
       " 98: 8,\n",
       " 101: 1}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2415beb3-da64-4686-8433-9ef2c2b965a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[92].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6490154-0bcb-4061-b252-4e24d404bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in count.items():\n",
    "    print("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
